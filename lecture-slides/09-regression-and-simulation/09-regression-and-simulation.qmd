---
title: "WEEK 9: LINEAR REGRESSION & SIMULATING DISTRIBUTIONS"
format: 
  revealjs:
    theme: [default, ../../slide_custom.scss]
    auto-stretch: false
editor: source
self-contained: true
execute:
  echo: true
---

```{r setup}
#| include: false
#| message: false
library(tidyverse)
```

## Monday, March 6th

Today we will...

+ Questions from Lab/Challenge 8
+ Final Project Part 2 -- Linear Regression
+ Mini lecture on text material
  + Review of Simple Linear Regression
  + Conditions of Linear Regression
+ PA 9.1: Mystery Animal
+ Lab 9: Baby Names


## NC Births Data

The `ncbirths` data set is a random sample of 1,000 cases taken from a larger 
data set collected in North Carolina in 2004. 

Each case describes the birth of a single child born in North Carolina, along
with various characteristics of the child (e.g. birth weight, length of
gestation, etc.), the child’s mother (e.g. age, weight gained during pregnancy,
smoking habits, etc.) and the child’s father (e.g. age). 

```{r}
library(openintro)
data(ncbirths)
head(ncbirths) |> 
  knitr::kable() |> 
  kableExtra::scroll_box(height = "200px")
```

## Relationships Between Variables

In a statistical model, we generally have one variable that is the output and
one or more variables that are the inputs.

:::: columns
::: column
+ Response variable
  + a.k.a. $y$, dependent
  + The quantity you want to understand
:::
::: column
+ Explanatory variable
  + a.k.a. $x$, independent, explanatory, predictor
  + Something you think might be related to the response
:::
::::

## Visualizing the Relationship

The scatterplot has been called the most "generally useful invention in the
history of statistical graphics."

:::: columns
::: column
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 4
#| fig-align: center
ggplot(data = bdims, aes(y = wgt, x = hgt)) + 
  geom_point() +
  scale_x_continuous("Explanatory Variable", labels = NULL) + 
  scale_y_continuous("Response Variable", labels = NULL) +
  theme(axis.title = element_text(size = 20)
        )
```
:::
::: column

**Characterizing relationships**

+ Form -- linear, quadratic, non-linear?

+ Direction -- positive, negative?

+ Strength -- how much scatter/noise?

+ Unusual observations -- do points not fit the overall pattern?
:::
::::

## Your turn!

How would your characterize this relationship? 

:::: {.columns}
::: {.column width="25%"}
- shape
- direction
- strength 
- outliers
:::
::: {.column width="75%"}
```{r}
#| echo: false
#| fig-height: 6
ncbirths |> 
ggplot(aes(x = weeks, y = weight)) +
  geom_jitter() + 
  labs(x = "Length of pregnancy (in weeks)",
       y = "Birth weight of baby (in lbs)") +
  theme(axis.title = element_text(size = 20),
        axis.text = element_text(size = 18)
        )
```
:::
::::

::: {.notes}
As we work through these, please keep in mind that much of what we are doing at
this stage involves making judgment calls. This is part of the nature of
statistics, and while it can be frustrating - especially as a beginner - it is
inescapable. For better or for worse, statistics is not a field where there is
one right answer. There are of course an infinite number of indefensible claims,
but many judgments are open to interpretation.

There isn’t a universal, hard-and-fast definition of what constitutes an outlier, but they are often easy to spot in a scatterplot.

+ What observations would you consider to be outliers?

+ How would you go about removing these outliers from the data?
:::

## Simple Linear Regression (SLR)

Assume the relationship between $x$ and $y$ takes the form of a linear function.

$$
  response = intercept + slope \cdot explanatory + noise
$$

:::: columns
::: column
**Population Regression Model**

$Y = \beta_0 + \beta_1 \cdot X + \epsilon_i$  

$\epsilon \sim N(0, \sigma)$
:::
::: column

**Fitted Regression Model (_Sample_)** 

$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 \cdot x$
:::
::::

::: {.notes}
Models are ubiquitous in statistics. In many cases, we assume that the value of
our response variable is some function of our explanatory variable, plus some
random noise.

What we are saying here is that there is some mathematical function $f$, which can translate values of one variable into values of another, except that there is some randomness in the process. What often distinguishes statisticians from other quantitative researchers is the way that we try to model that random noise.
:::


## Fitting an SLR Model 

::: panel-tabset

### `geom_smooth(method = "lm")`

```{r}
#| code-line-numbers: 6
#| fig-width: 8
#| fig-height: 4
#| fig-align: center
ncbirths |> 
ggplot(aes(x = gained, 
           y = weight)
       ) +
  geom_jitter() + 
  geom_smooth(method = "lm") 
```

### `lm`

```{r}
ncbirth_lm <- lm(weight ~ gained, 
                 data = ncbirths
                 )
```

### Coefficients

```{r}
library(broom)
tidy(ncbirth_lm)
```

+ Intercept: Expected __mean__ value for $y$, when $x$ is 0  

+ Slope: Expected change in the __mean__ of $y$, when $x$ is increased by 1 unit

### Residuals

Difference between the observed (point) and the expected (line).

```{r}
ncbirth_lm$residuals |> 
  head(3)

resid(ncbirth_lm) |> 
  head(3)

library(broom)
augment(ncbirth_lm) |> 
  head(3)
```

:::

## Model Diagnostics

::: panel-tabset

### Linearity

**Linear Relationship**

:::: {.columns}
::: {.column width="30%"}
+ Almost nothing you explore will look perfectly linear 

+ Be careful with relationships that have curvature 

+ Variable transformations can often help 
:::
::: {.column width="70%"}
```{r}
#| echo: false
#| fig-width: 10
#| fig-height: 6
library(patchwork)
## curvature
p1 <- ggplot(ncbirths, aes(x = weeks, y = weight)) +
  geom_jitter(alpha = 0.5) +
  geom_smooth(method = "lm") +
  labs(x = "Length of pregnancy (in weeks)", y = "Birth weight of baby (in lbs)")

p2 <- ggplot(ncbirths, aes(x = weeks, y = weight)) +
  scale_y_log10() +
  scale_x_log10() +
  geom_jitter(alpha = 0.5) +
  geom_smooth(method = "lm") +
  labs(x = "Length of pregnancy (in log weeks)", y = "Birth weight of baby (in log lbs)")

p1 + p2
```
:::
::::

### Independence

:::: columns
::: column
**Independence of observations**

Nearly every statistical model you will encounter, require for the observations in the data to be independent.

This is often the most crucial but also the most difficult to diagnose. 

<!-- It is also extremely difficult to gather a dataset which is a true random sample -->
<!-- from the population of interest.  -->

**What does independence mean?**

Independence means that I should not be able to know the $y$ value for one
observation based on knowing the $y$ value of another observation. 
:::
::: column
**Situations with independence violations**

If there is an inherent grouping of observations, then independence may be 
violated. 

+ Stock market prices over time  
+ Geographical similarities
+ Biological conditions of family members
+ Repeated observations 
:::
::::

### Normality

**Normally distributed residuals**

:::: columns
::: column
Observations vary symmetrically around the least squares line, spreading out
in a bell-shaped fashion.

Less important than linearity or independence for a few reasons: 

+ Least squares is an unbiased estimate of the true population model.
+ Larger sample sizes make the model more reliable. 
:::
::: column
```{r, fig.height = 6, fig.width = 6, echo = FALSE}
ncbirth_lm |> 
  augment() |> 
  ggplot(aes(x = .resid)) +
  geom_histogram(aes(y = ..density..)) +
  geom_density(color = "steelblue", 
               lwd = 1.5) + 
  xlab("Residuals")
```
:::
::::

### Equal Variance

**Equal (constant) variance**

:::: columns
::: column
- The variability of points around the least squares line remains roughly
constant.

- Data that exhibit non-equal variance across the range of x-values will have
the potential to seriously mis-estimate the variability of the slope. 

:::
::: column

```{r}
#| eval: false
ncbirth_lm |> 
  augment() |> 
ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() +
```

```{r, echo = FALSE, fig.height = 6, fig.width = 10}
p1 <- ncbirths |> 
  filter(weeks > 26) %>% 
  ggplot(aes(x = weeks, y = weight)) +
  geom_jitter() +
  geom_smooth(method = "lm") +
  labs(x = "Length of pregnancy (in weeks)", y = "Birth weight of baby (in lbs)") +
  theme(axis.title = element_text(size = 24),
        axis.text = element_text(size = 20)
        )

p2 <- ggplot(data = ncbirth_lm, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", lwd = 1.5) +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.title = element_text(size = 24),
        axis.text = element_text(size = 20)
        )

p1 + p2
```
:::
::::

:::

## Assessing Model Fit

:::: columns
::: column
```{r}
anova(ncbirth_lm)
summary(ncbirth_lm)
```
:::
::: column
+ **Sum of Square Errors (SSE)**
  + sum of squared residuals 

+ **Root Mean Square Error (RMSE)**
  + standard deviation of residuals 

+ **R-squared**
  + proportion of variability in response accounted for by the linear model
:::
::::

## Model Comparison

:::: columns
::: column
**Weight as explanatory variable**
```{r}
weight_weeks <- lm(weight ~ weeks, 
                   data = ncbirths
                   )
```

- SSE = 1246.55 
- RMSE = 1.119
- $R^2$ = 0.449
:::
::: column
**Visit as explanatory variable**
```{r}
weight_visits <- lm(weight ~ visits, 
                    data = ncbirths
                    )
```

- SSE = 2152.74
- RMSE = 1.475
- $R^2$ = 0.01819
:::
::::

## Extending to Multiple Linear Regression

If you have multiple explanatory variables you want to include in the model:

`lm(y ~ x1 + x2 + x3 + ... + xn)`

If you want to include interactions:

`lm(y ~ x1 + x2 + x1:x2)`

or use the shortcut:

`lm(y ~ x1*x2)`

You can include quadratic relationships (recall $ax^2 + bx + c$)

`lm(y ~ I(x1^2) + x1)`

## A quick note about piping `|>` and `_`

+ [The new R pipe](https://www.r-bloggers.com/2021/05/the-new-r-pipe/)
+ [Pipe updates: Placeholder](https://www.r-bloggers.com/2022/04/new-features-in-r-4-2-0/)

Instead of a `.x` you use `_`

```{r}
#| code-line-numbers: 4
weight_fullterm <- ncbirths |> 
  filter(premie == "full term") |> 
  lm(weight ~ weeks, 
     data = _
     )
```

## Worktime

+ [PA 9.1: Mystery Animal](https://earobinson95.github.io/stat331-calpoly/practice-activities/PA9.1-mystery-animal.html)

+ [Lab 9: Baby Names](https://earobinson95.github.io/stat331-calpoly/lab-assignments/lab9/Lab9-babynames.html)

+ [Challenge 9: Formatting Nice Tables](https://earobinson95.github.io/stat331-calpoly/lab-assignments/lab9/Challenge9-babynames.html)

## To do...
  
+ **PA 9.1: Mystery Animal**
  + Due **Wednesday, 3/8 at 8:00am**

+ **Lab 9: Baby Names**
  + Due **Friday, 3/10 at 11:59pm**
  
+ **Challenge 9: Formatting Nice Tables**
  + Due **Saturday, 3/11 at 11:59pm**
  
+ **Project Check-point 2: Linear Regression**
  + Due **Sunday, 3/12 at 11:59pm**

## Wednesday, March 8th

Today we will...

+ Mini lecture on text material
  + Simulating Distributions
+ Worktime
  + PA 9.2: Instrument Con
  + Lab & Challenge 9
  + Project: Linear Regression

## Statistical Distributions

Recall from your previous statistics classes...

::: panel-tabset

### Random Variable

A **random variable** is a value we don't know until we take a **sample**.

+ Coin flip:  Could be heads (0) or tails (1)
+ Person's height:  Could be anything from 0 feet to 20 feet.
+ Annual income of a US worker:  $0 to $1.6 billion

### Distribution

The **distribution** of the random variable tells us the **possible values** and **how likely they are**

:::: columns
::: column
+ Coin flip has 50% chance of heads, 50% chance of tails
+ Likelihood of heights follow a bell curve shape around 5 foot 7.
+ Most American workers make under $100,000
:::
::: column
```{r}
#| fig-align: center
#| echo: false
#| out-width: 70%
knitr::include_graphics("https://github.com/allisonhorst/stats-illustrations/blob/main/make-your-own-stats-cartoons/ex_3.png?raw=true")
```
:::
::::
    
:::

## Some distributions have names

For this class, you need to know a few of them.

::: panel-tabset

### `unif`

**Uniform Distribution**

:::: columns
::: column
When you know the **range**, but not much else.

All values in the range are **equally likely** to occur.
:::
::: column
```{r}
#| out-width: 100%
#| echo: false
my_samples <- data.frame(x = runif(1000))

ggplot(my_samples, aes(x)) + 
  geom_histogram(bins = 40, aes(y = ..density..), fill = "grey", col = "white") + 
  stat_function(fun=dunif, col = "cornflowerblue", lwd = 2) +
  theme_classic()
```
:::
::::
### `norm`

**Normal Distribution**

:::: columns
::: column
When you expect values to fall **near the center**.

Frequency of values follows a **bell curve** shape.
:::
::: column
```{r}
#| out-width: 100%
#| echo: false
my_samples <- data.frame(x = rnorm(1000))

ggplot(my_samples, aes(x)) + 
  geom_histogram(bins = 40, aes(y = ..density..), fill = "grey", col = "white") + 
  stat_function(fun=dnorm, col = "cornflowerblue", lwd = 2) +
  theme_classic()
```
:::
::::
### `t`

**t-Distribution**

:::: columns
::: column
Slightly **wider bell curve** shape.

Basically, the same context as the **Normal distribution**, but used more often in 
real data.  (When the *standard deviation* is unknown.)
:::
::: column
```{r}
#| out-width: 100%
#| echo: false

ggplot(my_samples, aes(x)) + 
  geom_histogram(bins = 40, aes(y = ..density..), fill = "grey", col = "white") + 
  stat_function(fun= function(x) dt(x, df = 5), col = "cornflowerblue", lwd = 2) +
  stat_function(fun = dnorm, col = "indianred", lwd = 1) +
  theme_classic()
```
:::
::::
### `chisq`

**Chi-Square Distribution**

:::: columns
::: column
Somewhat **skewed**, and only allows values **above zero**.

Used in testing **count data**.
:::
::: column
```{r}
#| out-width: 100%
#| echo: false
my_samples <- data.frame(x = rchisq(1000, df = 5))

ggplot(my_samples, aes(x)) + 
  geom_histogram(bins = 40, aes(y = ..density..), fill = "grey", col = "white") + 
  stat_function(fun= function(x) dchisq(x, df = 5), col = "cornflowerblue", lwd = 2) +
  theme_classic()
```
:::
::::

### `binom`

**Binomial Distribution**

:::: columns
::: column
Appears when you have **two possible outcomes**, and you are **counting**
how many times each outcome occurred.

(Common example: voting data)
:::
::: column
```{r}
#| out-width: 100%
#| echo: false
my_samples <- data.frame(x = rbinom(1000, 10, .8))

ggplot(my_samples, aes(x)) + 
  geom_bar(fill = "grey", col = "white") + 
  theme_classic()
```
:::
::::
:::

## Statistical Distributions

Things you might want to use a Statistical distribution for:

+ Find the **probability** of an **event**
  + If I flip 10 coins, what are the chances I get all heads?

+ Estimate a **percentage** of a **population**:

  + About what percent of people are above 6 feet tall?
  
+ Quantify the **evidence** in your **data**:
  + In my survey of 100 people, 67 said they were voting for Measure A.  How confident am I that Measure A will pass?

## Statistical Distributions in R -- `r`, `p`, `d`, `q`

::: panel-tabset
### `r` random sampling

+ `r` stands for for **random sampling**: generate a random value from the
distribution

+ We typically use this to **simulate** date; that is, to generate pretend observations
to see what happens.

:::: columns
::: column
```{r}
runif(n = 3)
runif(n = 3, min = 10, max = 20)

rnorm(n = 3)
rnorm(n = 3, mean = -100, sd = 50)
```
:::
::: column

```{r}

rt(n = 3, df = 11)

rbinom(n = 3, size = 10, prob = 0.7)

rchisq(n = 3, df = 11)
```
:::
::::

### `p` probability

+ `p` for **probability**: i.e. compute the chances of an observation less than `x`

+ We usually use this for calculating **p-values**

```{r}
pnorm(q = 1.5)
pnorm(q = 70, mean = 67, sd = 3)

1 - pnorm(q = 70, mean = 67, sd = 3)
pnorm(q = 70, mean = 67, sd = 3, lower.tail = FALSE)
```

### `q` quantile

+ `q` stands for **quantile**: given a probability p, compute the x-value
such that $$P(X < x) = p.$$

+ the `q` functions are the "backwards" version of the `p` functions

```{r}
qnorm(p = 0.95)
qnorm(p = 0.95, mean = 67, sd = 3)
```

### `d` density

`d` stands for **density**: compute the height of distribution curve

+ **Discrete** distributions: probability of getting that **exact value**

+ **Continuous** distributions: usually meaningless

What is the probability of getting exactly 12 heads in 20 coin tosses, for a
coin with a 70% chance of tails?

```{r}
dbinom(x = 12, size = 20, prob = 0.3)
```

:::

## Simulating Fake Data

::: panel-tabset
### `set.seed()`

Since there is randomness involved, if we want to create a reproducible random sample, we "set the seed" on so we get the same random sample each time the code is run.

```{r}
set.seed(94301)
```

### `tibble`
```{r}
#| code-line-numbers: 3-5 
fake_data <- tibble(
  names   = charlatan::ch_name(1000),
  height  = rnorm(1000, mean = 67, sd = 3),
  age     = runif(1000, min = 15, max = 75),
  measure = rbinom(1000, size = 1, prob = 0.6)
) |> 
  mutate(
    supports_measure_A = ifelse(measure == 1, "yes", "no")
  )

head(fake_data)
```

### visualize

```{r}
#| code-fold: true
#| fig-align: center
#| out-width: 70%
fake_data |> 
  ggplot(aes(y = supports_measure_A, 
             x = age,
             fill = supports_measure_A)
         ) +
  ggridges::geom_density_ridges(show.legend = F) +
  scale_fill_brewer(palette = "Paired") +
  theme_bw() +
  labs(x = "Age (years)",
       y = "",
       subtitle = "Support for Measure A",
       )
```

:::

## Plotting distributions

::: panel-tabset

### `geom_histogram`

```{r hist}
#| code-line-numbers: 3
#| fig-align: center
#| out-width: 70%
fake_data |> 
  ggplot(aes(x = height)) +
  geom_histogram(bins = 10, fill = "grey", color = "white") +
  theme_bw()
```

### `dnorm`

```{r}
#| code-line-numbers: 4
#| fig-align: center
#| out-width: 70%
fake_data |> 
  ggplot(aes(x = height)) +
  geom_histogram(bins = 10, fill = "grey", color = "white") +
  stat_function(fun = dnorm, 
                col = "steelblue", 
                lwd = 2
                )
```
### `dnorm(mean, sd)`

```{r}
#| code-line-numbers: 4
#| fig-align: center
#| out-width: 70%
fake_data |> 
  ggplot(aes(x = height)) +
  geom_histogram(bins = 10, fill = "grey", color = "white") +
  stat_function(fun = ~ dnorm(.x, mean = 67, sd = 3), 
                color = "steelblue", 
                lwd = 2
                )
```
### `..density..`

```{r}
#| code-line-numbers: 3
#| fig-align: center
#| out-width: 70%
fake_data |> 
  ggplot(aes(x = height)) +
  geom_histogram(aes(y = ..density..),
                 bins = 10, fill = "grey", color = "white") +
  stat_function(fun = ~ dnorm(.x, mean = 67, sd = 3), 
                color = "steelblue", 
                lwd = 2
                )
```
:::

## Empirical vs Theoretical Distributions

::: panel-tabset
### Empirical Distribution

```{r}
#| fig-align: center
#| out-width: 70%
#| code-fold: true
fake_data %>%
  ggplot(aes(x = height)) +
  geom_histogram(aes(y = ..density..), bins = 10) 
```

### Theoretical Distributions

```{r}
#| fig-align: center
#| out-width: 70%
#| code-fold: true
fake_data %>%
  ggplot(aes(x = height)) +
  stat_function(fun = ~dnorm(.x, mean = 67, sd = 3), 
                col = "steelblue", lwd = 2) +
  stat_function(fun = ~dnorm(.x, mean = 67, sd = 2), 
                col = "orange2", lwd = 2)
```
:::

## In-line code `` `r ` ``

When writing up a report which includes results from a random generation process, in order to ensure reproducibility in your document, use `r ` to include your output within your written description.

```{r}
my_rand <- rnorm(1, mean = 0, sd = 1)
my_rand
```

My random number is `r my_rand`.

## To do...

+ **PA 9.2: Instrument Con**
  + Due **Friday, 3/10 at 8:00am**
  
+ **Lab 9: Baby Names**
  + Due **Friday, 3/10 at 11:59pm**
  
+ **Challenge 9: Formatting Nice Tables**
  + Due **Saturday, 3/11 at 11:59pm**
  
+ **Project Check-point 2: Linear Regression**
  + Due **Sunday, 3/12 at 11:59pm**
  
+ Read **Chapter 10: Predictive Checks** and complete **Check-in 10.1**
  + Due **Monday, 3/13 at 8:00am**
  
  